setwd
setwd()
clean workspace
rm(list=ls())
# install required packages if needed
packages_needed <- c("multcomp")
for (package_name in packages_needed) {
if (!(package_name %in% rownames(installed.packages()))){
install.packages(package_name)
}
}
#===================
## Paper strength experiment
# Experimental parameters
alpha     <- 0.1
beta      <- 0.2
delta     <- 5
sigma.hat <- 6
# Read data
paper   <- read.table(file = "paper_strength.csv",
header = TRUE,
sep = ",")
summary(paper)
# First look at the data
# png(filename = "../figs/paperbox.png",
#     width = 600, height = 600,
#     bg = "transparent")
boxplot(TS_kPA~Hardwood,
data = paper,
xlab = "Hardwood",
ylab = "TS_kPA",
main = "Paper strength data",
pch  = 16,
col  = "gray")
# dev.off()
# Computational modeling and analysis
model <- aov(TS_kPA~Hardwood,
data = paper)
summary.aov(model)
# Check normality
shapiro.test(model$residuals)
library(car)
# png(filename = "../figs/paperqq.png",
#     width = 600, height = 600,
#     bg = "transparent")
qqPlot(model$residuals,
pch = 16,
lwd = 3,
cex = 2,
las = 1)
# dev.off()
# Check homoscedasticity
fligner.test(TS_kPA~Hardwood,
data = paper)
# png(filename = "../figs/papervar.png",
#     width = 600, height = 600,
#     bg = "transparent")
plot(x    = model$fitted.values,
y    = model$residuals,
cex  = 2,
las  = 1,
pch  = 16,
xlab = "Fitted values",
ylab = "Residuals")
grid(NULL,NULL, lwd=2, col = "#44444422")
# dev.off()
# Check independence
durbinWatsonTest(model)
# png(filename = "../figs/paperind.png",
#     width = 600, height = 600,
#     bg = "transparent")
plot(x    = seq_along(model$residuals),
y    = model$residuals,
type = "l",
las  = 1,
lwd  = 2,
lty  = 1,
xlab = "Residual order",
ylab = "Residual value")
points(x    = seq_along(model$residuals),
y    = model$residuals,
type = "p",
cex  = 2,
pch  = 16,
col  = as.numeric(paper[, 1]))
grid(NA,NULL, lwd=2, col = "#44444422")
# dev.off()
## Multiple comparisons
# Situation 1: all vs. all
library(multcomp)
paper_tukey <- glht(model,
linfct = mcp(Hardwood = "Tukey"))
paper_tukey_CI <- confint(paper_tukey,
level = 0.95)
# png(filename = "../figs/papertukey.png",
#     width = 600, height = 600,
#     bg = "transparent")
plot(paper_tukey_CI,
xlab       = "Tensile Strength (kPa)",
sub        = "- Hardwood data -",
cex.axis   = 1.2,
cex        = 2)
# dev.off()
# Situation 2: all vs. B
paper$Hardwood  <- relevel(paper$Hardwood,
ref = "B")
model2          <- aov(TS_kPA~Hardwood,
data = paper)
paper_dunnett   <- glht(model2,
linfct = mcp(Hardwood = "Dunnett"))
paper_dunnett_CI <- confint(paper_dunnett,
level = 0.95)
# png(filename = "../figs/paperdunnett.png",
#     width = 600, height = 600,
#     bg = "transparent")
plot(paper_dunnett_CI,
xlab       = "Tensile Strength (kPa)",
sub        = "- Hardwood data -",
cex.axis   = 1.2,
cex        = 2)
# dev.off()
#==========
## Sample size calculations for anova
a       <- 4
alpha   <- 0.05
sigma   <- 7
delta   <- 12
beta    <- 0.2
# Case 1: two levels symmetrically biased about the grand mean
tau <- c(-delta/2,
delta/2,
rep(0, a-2)) # define tau vector
n   <- 2        # initial n
while (qf(1 - alpha, a - 1, a*(n - 1)) >
qf(beta, a - 1, a*(n - 1), n*sum(tau^2)/sigma^2)) n <- n + 1
# Using power.anova.test():
vartau <- var(tau)
power.anova.test(groups = 4,
between.var = vartau,
within.var = sigma^2,
sig.level = alpha,
power = 1-beta)$n
# Case 2: one levels biased relative to the others
tau <- c(-delta*(a - 1)/a,
rep(delta/a, a-1)) # define tau vector
vartau <- var(tau)
power.anova.test(groups = 4,
between.var = vartau,
within.var = sigma^2,
sig.level = alpha,
power = 1-beta)$n
installed.packages("irace")
install.packages("irace")
install.packages("irace")
installed.packages("irace")
defaults write org.R-project.R force.LANG en_US.UTF-8
install.packages("irace")
install.packages("irace")
system("defaults write org.R-project.R force.LANG en_US.UTF-8")
packages_needed <- c("p.adjust")
for (package_name in packages_needed) {
if (!(package_name %in% rownames(installed.packages()))){
install.packages(package_name)
}
}
#call the necessary package
library(p.adjust)
library(MASS)
fix(Boston)
names(Boston)
summary(Boston)
?Boston
lm.fit = lm(medv~lstat)
attach(Boston)
lm.fit = lm(medv~lstat)
lm.fit
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
confint(lm.fit)
predict(lm.fit, data.frame(lstat=c(5,10,15)), interval = "confidence")
predict(lm.fit, data.frame(lstat=c(5,10,15)), interval = "prediction")
plot(lstat,medv)
plot(lstat,medv)
abline(lm.fit)
plot(lstat,medv)
abline(lm.fit)
abline(lm.fit,lwd = 3)
abline(lm.fit, lwd = 3, col = "red")
plot(lstat, medv, col = "red")
plot(lstat,medv, pch = 20)
plot(lstat,medv,pch="+")
plot(1:20,1:20,pch=1:20)
par(mfrow=c(2,2))
plot(lm.fit)
plot(predict(lm.fit), residuals(lm.fit))
the residuals
plot(predict(lm.fit), rstudent(lm.fit))
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
lm.fit = lm(medv~lstat+age,data = Boston)
summary(lm.fit)
riables
lm.fit = lm(medv~,data=Boston)
lm.fit = lm(medv~.,data=Boston)
summary(lm.fit)
summary(lm.fit)$r.sq
summary(lm.fit)$sigma
library(car)
install.packages("car")
library(car)
vif(lm.fit)
lm.fit1 = lm(medv~.-age, data =Boston)
summary(lm.fit1)
lm.fit1 = update(lm.fit,~.-age)
summary(lm(medv∼lstat*age,data=Boston))
summary(lm(medv∼lstat*age,data=Boston))
summary(lm(medv∼lstat * age,data=Boston)
summary(lm(medv~lstat*age,data = Boston))
lstat^2 using the I function call
lm.fit2 = lm(medv~lstat+I(lstat^2))
summary(lm.fit2)
lm.fit = lm(medv~lstat)
anova(lm.fit,lm.fit2)
par = (mfrow = c(2,2))
par (mfrow = c(2,2))
plot(lm.fit2)
ion with higher order polynomials using the poly() function
lm.fit5 = lm(medv~poly(lstat,5))
summary(lm.fit5)
summary(lm(medv∼log(rm),data=Boston))
summary(lm(medv~log(rm),data=Boston))
EDICTORS
names(Carseats)
names(Carseats)
library(ISLR)
summary(Carseats)
install.packages("ISLR")
library(ISLR)
summary(Carseats)
names(Carseats)
attach(Carseats)
lm.fit = lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)
#get the dummy vars
constrasts(ShelveLoc)
constrasts(shelveLoc)
contrasts(ShelveLoc)
library(ISLR)
names(Smarket)
dim(Smarket)
summary(Smarket)
pairs(Smarket)
cor(Smarket)
cor(Smarket[,-9])
plot(Volume)
attach(Smarket)
plot(Volume)
plot(Volume, color = 'red')
plot(Volume, col = "red")
plot(Volume, col = 2)
#do a logistic regression with lag and volume to predict direction
glm.fits = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data = Smarket, family = binomial)
summary(glm.fits)
coef(glm.fits)
glm.probs = predict(glm.fits, type = "response")
glm.probs[1:10]
contrasts(Direction)
glm.pred = rep("Down",1250)
glm.pred[glm.probs > 0.5] = "Up"
table(glm.pred, Direction)
(507+145)/1250
mean(glm.pred == Direction)
train = (Year<2005)
Smarket.2005 = Smarket[!train,]
glm.fits = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data = Smarket.2005, family = binomial)
summary(glm.fits)
dim(glm.fits)
dim(Smarket.2005)
dim(Smarket.2005)[0]
dim(Smarket.2005)[[0]]
dim(Smarket.2005)
rows(Smarket.2005)
row(Smarket.2005)
nrow(Smarket.2005)
glm.pred = rep("Down",nrow(Smarket.2005))
glm.pred[glm.probs >.5]="Up"
table(glm.pred, Direction.2005)
Direction.2005=Direction[!train]
table(glm.pred, Direction.2005)
glm.pred = rep("Down",nrow(Smarket.2005))
glm.pred[glm.probs >.5]="Up"
glm.probs = predict(glm.fits, Smarket.2005, type = "response")
glm.pred = rep("Down",nrow(Smarket.2005))
glm.pred[glm.probs >.5]="Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
mean(glm.pred != Direction.2005)
glm.fits=glm(Direction∼Lag1+Lag2,data=Smarket ,family=binomial, subset=train)
glm.fits=glm(Direction~Lag1+Lag2,data=Smarket ,family=binomial, subset=train)
glm.probs=predict(glm.fits,Smarket.2005,type="response")
glm.pred=rep("Down",252)
glm.pred[glm.probs >.5]="Up"
table(glm.pred,Direction .2005)
table(glm.pred,Direction.2005)
mean(glm.pred==Direction .2005)
mean(glm.pred==Direction.2005)
predict(glm.fits,newdata=data.frame(Lag1=c(1.2,1.5), Lag2=c(1.1,-0.8)),type="response")
library(MASS)
lda.fit = lda(Direction~Lag1+Lag2, data = Smarket, subset = train)
lda.fit
plot(lda.fit)
lda.pred = predict(lda.fit,Smarket.2005)
names(lda.pred)
lda.class = lda.pred$class
table(lda.class, Direction.2005)
mean(lda.class == Direction.2005)
sum(lda.pred$posterior[,1]>=.5)
sum(lda.pred$posterior[,1]<.5)
lda.pred$posterior[1:20,1]
lda.class[1:20]
sum(lda.pred$posterior[,1]>=.9)
qda.fit = qda(Direction~Lag1+Lag2, data=Smarket, subset = train)
qda.fit
qda.class = predict(qda.fit, Smarket.2005)$class
table(qda.class, Direction.2005)
qda.class
mean(qda.class == Direction.2005)
library(class)
train.X = cbind(Lag1, Lag2)[train,]
View(train.X)
test.X = cbind(Lag1, Lag2)[!train,]
train.Direction = Direction[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k =1)
table(knn.pred, Direction.2005)
knn.pred = knn(train.X, test.X, train.Direction, k = 3)
table(knn.pred, Direction.2005)
mean(knn.pred == Direction.2005)
ion to the caravan insurance data
dim(Caravan)
attach(Caravan)
summary(Purchase)
standardized.X = scale(Caravan[-86,])
standardized.X = scale(Caravan[,-86])
var(Caravan[,1])
var(Caravan[,2])
var(standardized.X[,1])
test = 1:1000
train.X = standardized.X[-test,]
test.X = standardized.X[test,]
train.Y = Purchase[-test]
test.Y = Purchase[test]
knn.pred = knn(train.X, test.X, train.Y, k=1)
mean(test.Y!= knn.pred)
mean(test.Y!="No")
table(knn.pred,test.Y)
packages_needed <- c('MASS','tolerance')
for (package_name in packages_needed) {
if (!(package_name %in% rownames(installed.packages()))){
install.packages(package_name)
}
}
for (i in 1:length(packages_needed))
{
library(packages_needed[[i]],character.only=TRUE)
}
setwd("~/Dropbox/Doutorado/R-repo")
Heart <- read.csv('heart.csv')
setwd("~/Dropbox/Doutorado/R-repo/Datasets")
setwd("~/Dropbox/Doutorado/R-repo/Datasets")
Heart <- read.csv('heart.csv')
dim(Heart)
names(Heart)
summary(Heart)
any(is.na(Heart)) #check for empty rows
Heart$Age
mean(Heart$Age)
pairs(Heart) #plots of everything
cp<- as.factor(Heart$Cp)
plot(Heart$Cp, Heart$Oldpeak)
plot(cp, Heart$Oldpeak)
nm <- read.table("NelderMead.txt", header = TRUE, sep = ",", dec = ".")
dim(nm)
names(nm)
head(nm)
unique(nm$dimension)
unique(nm$method_start)
summary(nm)
unique(nm$dimension)
unique(nm$a)
unique(nm$s1)
unique(nm$fx)
View(nm)
View(nm)
unique(Heart$Cp)
unique(Heart$Oldpeak)
unique(Heart$Age)
unique(Heart$Sex)
#dice experiment
for (val in c(100,1000,10000,100000)){
rolls <- floor(runif(val,min=1, max = 7))
#histogram
hist(rolls, main = paste("Histogram of", val, "rolls"), col = 'green', las=1, breaks = c(0,1,2,3,4,5,6))
}
rnorm(10,mean=0,sd=1)
val <- rnorm(10000,mean=0,sd=1)
hist(val)
val <- rnorm(10,mean=0,sd=1)
hist(val)
val <- rnorm(100,mean=0,sd=1)
hist(val)
val <- rnorm(1000,mean=0,sd=1)
hist(val)
View(Heart)
#load dataset
hp <- read.csv("HPressure.csv")
hp$intval
error<- qnorm(0.975)*1/sqrt(nrow(hp)) #std is 1 in this case
#right side of the distrib
right<- mean(hp$intval) + error
#left side of the distrib
left<- mean(hp$intval) - error
right
left
error<- qnorm(0.975,df=nrow(hp)-1)*sd(hp$intval)/sqrt(nrow(hp)) #n-1 degrees of freedom (t-distribution)
#right side of the distrib
right<- mean(hp$intval) + error
#left side of the distrib
left<- mean(hp$intval) - error
right
left
tol <- normtol.int(x = hp$intval, alpha = 0.05, P = 0.95, side = 2)
tol
#interval for plotting
x<- seq(110,130)
#drawing a normal distribution from our sample norm and sd
hx<- dnorm(x, mean = mean(hp$intval), sd = sd(hp$intval))
plot(x,hx, type = "l", lty = 2, xlab = "x value", ylab = "Density")
abline(v = tol$`2-sided.lower`, col = 'red') #put a vertical line at the left side of the tolerance interval
abline(v = tol$`2-sided.upper`, col = 'red') #put a vertical line at the right side of the tolerance interval
packages_needed <- c('car', 'nortest')
for (package_name in packages_needed) {
if (!(package_name %in% rownames(installed.packages()))){
install.packages(package_name)
}
}
for (i in 1:length(packages_needed))
{
library(packages_needed[[i]],character.only=TRUE)
}
#load dataset
hp <- read.csv("HPressure.csv")
#do the t-test on the blood pressure dataset
t.test(hp$intval, mu = 120, conf.level =0.95)
#power t-test to check for power
power.t.test(n = 10, delta = 1.2, sd = sd(hp$intval), sig.level = 0.01, type = "one.sample")
#power t-test to determine sample size
power.t.test(power = 0.85, delta = 1.2, sd = sd(hp$intval), sig.level = 0.01, type = "one.sample")
#qqplot
qqPlot(hp$intval, pch = 16, cex = 1, las = 1, col = 'red')
#anderson darling (requires nortest lib)
ad.test(hp$intval)
#shapiro wilk (requires car lib)
shapiro.test(hp$intval)
#Kolmogorov-Smirnov test (does our sample follow a normal distrib with that sample mean?)
ks.test(hp$intval, 'pnorm', mean(hp$intval))
durbinWatsonTest(lm(hp$intval ~1))
#load the dataset
data <- read.csv('HPressure_multiple')
View(data)
#pooled t-test
t.test(data$control, data$test, var.equal = TRUE, conf.level = 0.95)
#paired t-test
t.test(data$control, data$test, var.equal = TRUE, conf.level = 0.95, paired = TRUE)
#NORMALITY TEST FOR THE DATA
par(mfrow=c(1,2))
qqPlot(df$control, cex=1.5,pch=16, las=1)
qqPlot(df$test, cex=1.5,pch=16, las=1)
df <- read.csv('HPressure_multiple')
par(mfrow=c(1,2))
qqPlot(df$control, cex=1.5,pch=16, las=1)
qqPlot(df$test, cex=1.5,pch=16, las=1)
#shapiro-wilk test
#list of results
res_shapiro <- list("control" = NULL, "test" = NULL)
for(c in colnames(df)){
res_shapiro[[c]] <- shapiro.test(df[[c]])
}
res_shapiro['control']
res_shapiro['test']
#HETEROSCEDASCITY
#fligner kileen test
fligner.test(df)
#plot our residuals
resids <- apply(df, 1, function (x) (x- mean(x)))
resids <- list("test" = resids[1,], "control" = resids[2,])
stripchart(x = resids,
vertical =  TRUE,
pch = 16,
cex = 1.5,
las= 1,
xlab = 'mean',
ylab = 'residuals')
#durbin watson for two samples
durbinWatsonTest(lm(df$control + df$test ~1))
