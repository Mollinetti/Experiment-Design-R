qda.class
mean(qda.class == Direction.2005)
library(class)
train.X = cbind(Lag1, Lag2)[train,]
View(train.X)
test.X = cbind(Lag1, Lag2)[!train,]
train.Direction = Direction[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k =1)
table(knn.pred, Direction.2005)
knn.pred = knn(train.X, test.X, train.Direction, k = 3)
table(knn.pred, Direction.2005)
mean(knn.pred == Direction.2005)
ion to the caravan insurance data
dim(Caravan)
attach(Caravan)
summary(Purchase)
standardized.X = scale(Caravan[-86,])
standardized.X = scale(Caravan[,-86])
var(Caravan[,1])
var(Caravan[,2])
var(standardized.X[,1])
test = 1:1000
train.X = standardized.X[-test,]
test.X = standardized.X[test,]
train.Y = Purchase[-test]
test.Y = Purchase[test]
knn.pred = knn(train.X, test.X, train.Y, k=1)
mean(test.Y!= knn.pred)
mean(test.Y!="No")
table(knn.pred,test.Y)
load("~/Documents/Projects/R/ISLR/data/Hitters.rda")
attach(Hitters)
alary
names(Hitters)
dim(Hitters)
sum(is.na(Salary))
Hitters = na.omit(Hitters)
install.packages("leaps")
library(leaps)
regfit.full = regsubsets(Salary~., Hitters)
summary(regfit.full)
regfit.full = regsubsets(Salary~., Hitters, nvmax = 19)
summary(regfit.full)
names(regfit.full)
reg.summary = summary(regfit.full)
names(reg.summary)
reg.summary$rsq
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",type="1")
plot(reg.summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq",type="1")
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",type="l")
plot(reg.summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq",type="l")
par = (mfrow = c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",type="l")
plot(reg.summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq",type="l")
which.max(reg.summary$adjr2)
which.max(reg.summary$adjr2)
#highlight point 11 at the graph
points(11,reg.summary$adjr2[11], col = "red", cex = 2, pch = 20)
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type=’l’)
#minimum point of cp
which.min(reg.summary$cp )
points(10,reg.summary$cp [10],col="red",cex=2,pch=20)
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type=’l’)
#minimum point of cp
which.min(reg.summary$cp )
points(10,reg.summary$cp [10],col="red",cex=2,pch=20)
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type=’l’)
#minimum point of cp
which.min(reg.summary$cp )
points(10,reg.summary$cp [10],col="red",cex=2,pch=20)
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type=’l’)
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type="l")
which.min(reg.summary$cp)
points(10,reg.summary$cp [10],col="red",cex=2,pch=20)
which.min(reg.summary$bic )
plot(reg.summary$bic ,xlab="Number of Variables ",ylab="BIC",type="l")
points(6,reg.summary$bic [6],col="red",cex=2,pch=20)
#perform best subset selection
regfit.full = regsubsets(Salary~., Hitters)
summary(regfit.full)
#fitting a model with up to 19 variables
regfit.full = regsubsets(Salary~., Hitters, nvmax = 19)
reg.summary = summary(regfit.full)
#get the statistics from reg.summary
names(reg.summary)
#check the R^2 statistic (note that it monotonically increases with the number of variables)
reg.summary$rsq
#plot RSS, adjusted R^2, CP and BIC
par = (mfrow = c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",type="l")
plot(reg.summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq",type="l")
#check the largest adjusted R^2 statistic
which.max(reg.summary$adjr2)
#highlight point 11 at the graph
points(11,reg.summary$adjr2[11], col = "red", cex = 2, pch = 20)
#plot the cp
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type="l")
#minimum point of cp
which.min(reg.summary$cp)
points(10,reg.summary$cp [10],col="red",cex=2,pch=20)
#minimum point of bic
which.min(reg.summary$bic )
#plot the bic
plot(reg.summary$bic ,xlab="Number of Variables ",ylab="BIC",type="l")
points(6,reg.summary$bic [6],col="red",cex=2,pch=20)
plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")
coef(regfit.full,6)
regfit.fwd = regsubsets(Salary~., Hitters, nvmax = 19, method = "forward")
#with backward
regfit.bwd = regsubsets(Salary~., Hitters, nvmax = 19, method = "backward")
regfit.fwd = regsubsets(Salary~., Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)
#with backward
regfit.bwd = regsubsets(Salary~., Hitters, nvmax = 19, method = "backward")
summary(regfit.bwd)
coef(regfit.full ,7)
coef(regfit.fwd ,7)
coef(regfit.bwd ,7)
train = sample(c(TRUE,FALSE), nrow(Hitters), rep = TRUE)
test = (!train)
regfit.best = regsubsets(Salary~., data = Hitters[train,], nvmax = 19)
test.mat =  model.matrix(Salary~. data = Hitters[test,])
test.mat =  model.matrix(Salary~., data = Hitters[test,])
View(test.mat)
val.errors = rep(NA,19)
val.errors = rep(NA,19)
for (i in 1:19)
{
coefi = coef(regfit.best, id  = i)
pred = test.mat[,names(coefi)]%*%coefi
val.errors[i] = mean((Hitters$Salary[test] - pred)^2)
}
val.errors
which.min(val.errors)
set.seed(1)
#get a random vector of training
train = sample(c(TRUE,FALSE), nrow(Hitters), rep = TRUE)
test = (!train)
#regsubsets to the training set
regfit.best = regsubsets(Salary~., data = Hitters[train,], nvmax = 19)
#we first make a model matrix from the test data
test.mat =  model.matrix(Salary~., data = Hitters[test,])
#now we run a loop and for each size i we extract the coefficients from regfit.best for the best model of that size
#multiply them into the appropriate columns of the test model matrix to form the predictions and computer the test MSE
val.errors = rep(NA,19)
for (i in 1:19)
{
coefi = coef(regfit.best, id  = i)
pred = test.mat[,names(coefi)]%*%coefi
val.errors[i] = mean((Hitters$Salary[test] - pred)^2)
}
val.errors
which.min(val.errors)
predict.regsubsets = function (object ,newdata ,id ,...)
{
form=as.formula(object$call [[2]])
mat=model.matrix(form,newdata)
coefi=coef(object ,id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
regfit.best=regsubsets(Salary∼.,data=Hitters ,nvmax=19)
coef(regfit.best ,10)
regfit.best=regsubsets(Salary~.,data=Hitters ,nvmax=19)
coef(regfit.best ,10)
folds = sample(1:k,nrow(Hitters), replace = TRUE)
k = 10
set.seed(1)
folds = sample(1:k,nrow(Hitters), replace = TRUE)
folds
cv.errors=matrix(NA,k,19, dimnames=list(NULL, paste(1:19)))
for (j in 1:k)
{
best.fit = regsubsets(Salary~., data = Hitters[folds!=j,], nvmax = 19)
for (i in 1:19)
{
pred = predict(best.fit, Hitters[folds == j], id = i)
cv.errors[j,i] = mean((Hitters$Salary[folds == j] - pred)^2)
}
}
k = 10
set.seed(1)
folds = sample(1:k,nrow(Hitters), replace = TRUE)
cv.errors=matrix(NA,k,19, dimnames=list(NULL, paste(1:19)))
#write a loop that does the cross validation
for (j in 1:k)
{
best.fit = regsubsets(Salary~., data = Hitters[folds!=j,], nvmax = 19)
for (i in 1:19)
{
pred = predict(best.fit, Hitters[folds == j], id = i)
cv.errors[j,i] = mean((Hitters$Salary[folds == j] - pred)^2)
}
}
#write a loop that does the cross validation
for (j in 1:k)
{
best.fit = regsubsets(Salary~., data = Hitters[folds!=j,], nvmax = 19)
for (i in 1:19)
{
pred = predict(best.fit, Hitters[folds == j,], id = i)
cv.errors[j,i] = mean((Hitters$Salary[folds == j] - pred)^2)
}
}
View(cv.errors)
mean.cv.errors=apply(cv.errors ,2,mean)
par(mfrow=c(1,1))
plot(mean.cv.errors ,type=’b’)
par(mfrow=c(1,1))
plot(mean.cv.errors ,type="b")
#the cross validation indicates that a 11 variable model is the best. We do best subset selection on a 11 variable model
reg.best=regsubsets (Salary∼.,data=Hitters , nvmax=19)
coef(reg.best ,11)
reg.best=regsubsets (Salary∼.,data=Hitters , nvmax=19)
reg.best=regsubsets (Salary~.,data=Hitters , nvmax=19)
coef(reg.best ,11)
x=model.matrix(Salary∼.,Hitters)[,-1]
y=Hitters$Salary
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary
library(glmnet)
install.packages('glmnet')
library(glmnet)
grid=10^seq(10,-2,length=100)
#alpha = 0 is ridge/ alpha = 1 is lasso
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
dim(coef(ridge.mod))
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
ridge.mod$lambda [60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
predict(ridge.mod,s=50,type="coefficients")[1:20,]
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh =1e -12)
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh =1e-12)
ridge.pred = predict(ridge.mod, s=4, newx = x[test,])
mean((ridge.pred - y.test)^2)
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T)
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=TRUE)
set.seed (1)
cv.out=cv.glmnet(x[train ,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda .min
bestlam=cv.out$lambda.min
bestlam
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.mod=glmnet(x[train ,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
set.seed (1)
cv.out=cv.glmnet(x[train ,],y[train],alpha=1)
plot(cv.out)
#get the best lambda
bestlam=cv.out$lambda.min
#predict the partial data set with the lambda
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.coef
lasso.coef[lasso.coef!=0]
library(pls)
install.packages('pls')
#PCR AND PLS REGRESSION
library(pls)
set.seed(2)
pcr.fit = pcr(Salary~., data = Hitters, scale = TRUE, validation = "CV")
summary(pcr.fit)
validationplot(pcr.fit,val.type="MSEP")
set.seed (1)
pcr.fit=pcr(Salary∼., data=Hitters,subset=train,scale=TRUE, validation ="CV")
pcr.fit=pcr(Salary~., data=Hitters,subset=train,scale=TRUE, validation ="CV")
validationplot(pcr.fit,val.type="MSEP")
pcr.pred=predict(pcr.fit,x[test,],ncomp=7)
mean((pcr.pred-y.test)^2)
pcr.fit=pcr(y∼x,scale=TRUE,ncomp=7)
pcr.fit=pcr(y~x,scale=TRUE,ncomp=7)
summary(pcr.fit)
a = 5
if (a < 10)
{
print("small number")
}
a = 5
if (a <= 4){
print("small number")
}
else{
print("big number")
}
a = 5
if (a <= 4){
print("small number")
}
else{
print("big number")
}
if (a <= 4){
print("small number")
else{
print("big number")
}
if (a <= 4){
print("small number")
}
else{
print("big number")
}
if (a <= 4){
print("small number")
}
else{
print("big number")
}
if (a <= 4){
print("small number")
}
else{
print("big number")
}
if (a <= 4){
print("small number")
}
else{
print("big number")
}
if (a <= 4){
print("small number")
}
else{
print("big number")
}
if (a <= 4){
print("small number")
}
else{
print("big number")
}
if (a <= 4){
print("small number")
}
else if (a > 4 && a <=10)
{
print("medium number")
}
if (a <= 4){
print("small number")
}
else if (a > 4 && a <=10)
{
print("medium number")
}
else{
print("big number")
}
if (a <= 4){
print("small number")
}
else if (a > 4 && a <=10)
{
print("medium number")
}
else
print("big number")
if (a <= 4){
print("small number")
}
else if (a > 4 && a <=10)
{
print("medium number")
}
else(){
print("big number")
}
if (a <= 4){
print("small number")
}
else if (a > 4 && a <=10){
print("medium number")
}
else{
print("big number")
}
if (a <= 4){
print("small number")
}
else if (a > 4 && a <=10){
print("medium number")
}
else{
print("big number")
}
if (a <= 4){
print("small number")
}
else if (a > 4 && a <=10){
print("medium number")
}
else{
print("big number")
}
if (a <= 4){
print("small number")
}
else if (a > 4 && a <=10){
print("medium number")
}
else{
print("big number")
}
if (a <= 4){
print("small number")
}
else if (a > 4 && a <=10){
print("medium number")
}
else{
print("big number")
}
if (a <= 4){
print("small number")
} else if (a > 4 && a <=10){
print("medium number")
} else{
print("big number")
}
if (a <= 4){
print("small number")
} else{
print("big number")
}
a = 5
b = TRUE
if (a <= 4){
if(b==TRUE){
print("a is small and b is true")
} else{
print("a is small and b is false")
}
} else{
if(b==TRUE){
print("a is big and b is true")
} else{
print("a is big and b is false")
}
}
i = 0
while(i < 4){
print(i)
i = i + 1
}
x <- c(2,5,3,9,8,11,6)
count <- 0
for (val in x) {
if(val %% 2 == 0)  count = count+1
}
print(count)
x <- c(2,5,3,9,8,11,6)
count <- 0
for (val in x) {
if(val %% 2 == 0){
count = count+1
}
}
print(count)
x <- 1:3
y<- 1:2
for (val1 in x) {
for (val2 in y){
print(c(val1,val2))
}
}
install.packages('car')
library(car)
setwd("~/Dropbox/Doutorado/R-repo/Datasets")
load("~/Dropbox/Doutorado/R-repo/Datasets/Auto.rda")
attach(Auto)
names(Auto)
summary(Auto)
plot(Auto$cylinders, Auto$mpg)
plot(cylinders,mpg)
cylinders = as.factor(cylinders)
plot(cylinders,mpg)
plot(acceleration,year)
plot(https://www.dropbox.com/s/c2p7mea7jyltot7/Screenshot%202019-09-05%2017.31.24.png?dl=0,acceleration)
plot(year,acceleration)
plot(mpg,cylinders)
plot(horsepower,weight)
plot(horsepower,weight)
load("~/Dropbox/Doutorado/R-repo/Datasets/Auto.rda")
plot(cylinders,mpg)
load("~/Dropbox/Doutorado/R-repo/Datasets/Auto.rda")
plot(cylinders,mpg)
cylinders = as.factor(cylinders)
plot(cylinders,mpg)
plot(cylinders,mpg, col = "red", varwidth = T, horizontal = T, xlab = "cylinders", ylab = "MPG")
plot(cylinders,mpg)
plot(cylinders,mpg, col = "red", varwidth = T, horizontal = T)
plot(cylinders,mpg, col = "red", varwidth = T)
plot(cylinders,mpg, col = "red", varwidth = T, xlab = "cylinders", ylab = "MPG")
plot(cylinders,mpg, col = "red", varwidth = T, xlab = "cylinders", ylab = "MPG")
